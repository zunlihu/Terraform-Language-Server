# -*- coding: utf-8 -*-
import sys
import getopt
from urllib.request import urlopen
from urllib.request import Request
from urllib.request import urlretrieve
import datetime
import json
import math
import argparse
import os
count=1
url_list = []

def get_results(url,headers):
    try:
        req =  Request(url,headers=headers)
        response = urlopen(req).read()
        results = json.loads(response.decode())
        return results
    except:
        return []

def download(url, save_path, fileExtension):
    global count
    global url_list
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    try:
        urlretrieve(url,save_path+str(count)+fileExtension)
        print ('No.%s'%str(count))
        print ('Download successfully:%s\n'%(url))
        count += 1
        url_list.append(url)
        
    except:
        print ('Fail to download the tf file:%s\n'%(url))

def download_files(folder,headers,save_path, fileExtension):
    try:
        files = get_results(folder,headers)
        for file in files:
            if fileExtension in  file['name']:
                download(file['download_url'],save_path, fileExtension)
        #elif (not "." in file['name']) and isinstance(get_results(folder+'/'+file['path'],headers),list):# if is subfolder
        #    download_files(folder+'/'+file['path'],headers)
    except:
        print('Download Failed!')

def main():
    global count
    global url_list
    ########################################## Argument #########################################
    query = "terraform"
    language = ""
    sort = ""
    order = ""
    size = ""
    #token = "0f3a05b400382b2839c87e19de1432dd35f456fa"
    token = ""
    save_path = './'
    fileExtension = '.txt'

    parser = argparse.ArgumentParser()
    parser.add_argument('--token', type=str, default=token, help='(Required) Authentication token generated by github account')
    parser.add_argument('--save_path', type=str, default=save_path, help='(Optional) Define the folder to save files. If not exist, create it.')
    parser.add_argument('--fileExtension', type=str, default=fileExtension, help='(Required) File extension to save file. Default id .txt')
    parser.add_argument('--query', type=str, default=query, help='(Required) Search Key word used in query github repositories.')
    parser.add_argument('--language', type=str, default=language, help='(Optional) Searches repositories based on the language written in.')
    parser.add_argument('--sort', type=str, default=sort, help='(Optional) The sort field. One of stars, forks, or updated. Default: results are sorted by best match.')
    parser.add_argument('--order', type=str, default=order, help='(Optional) Define the sorting order, e.g. desc or asc')
    parser.add_argument('--size', type=str, default=size, help='(Optional) Finds repositories that match a certain size (in kilobytes).')

    args = parser.parse_args()

    if args.token:
        token = args.token
    if args.query:
        query = args.query
    if args.save_path:
        save_path = args.save_path
    if args.fileExtension:
        fileExtension = args.fileExtension
    if args.language:
        language = "language:" + args.language
    if args.sort:
        sort = "sort:" + args.sort
    if args.order:
        order = "order:" + args.order
    if args.size:
        size = "size:" + args.size

    ########################################## Main #########################################
    headers = {'User-Agent': 'Mozilla/5.0',
               'Authorization': 'token %s'%token,
               'Content-Type': 'application/json',
               'Accept': 'application/json'
    }

    #total_repos = 8591
    url = 'https://api.github.com/search/repositories?q='+query+language+sort+order+size
    print(url)
    repos = get_results(url,headers)
    if(repos == []):
        print("No Searching REPOs!")
        return

    total_repos = repos['total_count']
    print ('total_repos:%s'%total_repos)
    total_page = math.ceil(total_repos/100)
    print ('total page:%s'%str(total_page))
    for page in range(1,total_page+1):
        page_url = url+'&page=%s&per_page=100'%(str(page))
        print(page_url)
        results = get_results(page_url,headers)   
        print(results)
        try:  
            for item in results['items']:
                name = item['name']
                owner = item['owner']['login']
                contents_url = 'https://api.github.com/repos/{owner}/{name}/contents'.format(owner=owner, name=name)
                contents = get_results(contents_url,headers)
                for file in contents:
                    try:
                        if fileExtension in  file['name']:
                            print ('Page%s:'%str(page))
                            download(file['download_url'],save_path, fileExtension)
                        elif isinstance(get_results(contents_url+'/'+file['path'],headers),list):# if is subfolder
                            subfolder = contents_url+'/'+file['path']
                            files = get_results(subfolder,headers)
                            for code in files:
                                if fileExtension in  code['name']:
                                    print ('Page%s:'%str(page))
                                    download(code['download_url'],save_path, fileExtension)
                    except:
                        print('Cannot download!')              
        except:     
            print('Open Error!')
    print('Download Done!\n')  

    print('Start write url list into txt...\n')        
    try:
        fp = open(save_path + 'url_list.txt', 'a')  
        for line in url_list:
            fp.write(line+'\n')
        fp.close()    
        print ('Write Done!')
    except:
        print('Write Error!')
        
if __name__ == '__main__':
    main()
